{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Useful Libraries and files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Import Useful Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use command `pip install -r requirement.txt` to install all the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Import Some Useful Functions From [TensorFlow Examples](https://github.com/tensorflow/examples/tree/master/lite/examples/pose_estimation/raspberry_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to clone the following [repository](https://github.com/tensorflow/examples/tree/master), and then change the `pose_sample_rpi_path` to the path of your cloned repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_sample_rpi_path = r'C:\\Users\\Administrator\\Desktop\\Deeplearning\\examples\\lite\\examples\\pose_estimation\\raspberry_pi'\n",
    "sys.path.append(pose_sample_rpi_path)\n",
    "\n",
    "# Load MoveNet Thunder model\n",
    "import utils\n",
    "from data import BodyPart\n",
    "from ml import Movenet\n",
    "movenet = Movenet('movenet_thunder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Image Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to use the image scaling function below, make sure your own dataset folder is structured as follows:\n",
    "Dataset\n",
    "    |__bad\n",
    "        |__001.jpg\n",
    "        |__002.jpg\n",
    "        |__......\n",
    "    |__good\n",
    "        |__001.jpg\n",
    "        |__002.jpg\n",
    "        |__......\n",
    "\"\"\"\n",
    "\n",
    "def resize_image(image_path, output_path, base=256):\n",
    "    \"\"\"\n",
    "    Adjust the resolution of the image and then save to the target folder\n",
    "    \n",
    "    Args: \n",
    "        image_path: the path of the image\n",
    "        Output_path: the output path of the proccessed image\n",
    "        base: If the length and width of the image are not the same, the longer side is scaled to this value.\n",
    "        \"\"\"\n",
    "    with Image.open(image_path) as img:\n",
    "        # calculate the new width and length to keep the ratio\n",
    "        w, h = img.size\n",
    "        if w > h:\n",
    "            new_w = base\n",
    "            new_h = int(base * h / w)\n",
    "        else:\n",
    "            new_h = base\n",
    "            new_w = int(base * w / h)\n",
    "\n",
    "        # Adjust the reso and then save the image\n",
    "        try:\n",
    "            img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "        except AttributeError:\n",
    "            img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "        img.save(output_path)\n",
    "\n",
    "def process_images(source_folder, target_folder):\n",
    "    \"\"\"\n",
    "    Iterate over all images in the source folder and adjust their resolution\n",
    "    This function will generate bad_resized and good_resized folders in the destination folder of your choice to store the resized images.\n",
    "    \"\"\"\n",
    "    for subdir in os.listdir(source_folder):\n",
    "        subdir_path = os.path.join(source_folder, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            target_subdir = os.path.join(target_folder, subdir + '_resize')\n",
    "            if not os.path.exists(target_subdir):\n",
    "                os.makedirs(target_subdir)\n",
    "\n",
    "            for image_name in os.listdir(subdir_path):\n",
    "                source_path = os.path.join(subdir_path, image_name)\n",
    "                target_path = os.path.join(target_subdir, image_name)\n",
    "\n",
    "                #  Adjust image resolution and save to destination folder\n",
    "                resize_image(source_path, target_path)\n",
    "                print(f'Processed and saved: {target_path}')\n",
    "\n",
    "# source folder path\n",
    "source_folder = 'Your source folder path'\n",
    "\n",
    "# target folder path\n",
    "target_folder = 'Your target folder path'\n",
    "\n",
    "process_images(source_folder, target_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Image Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def pad_image_to_square(image, fill_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Pads an image to make it square while maintaining its aspect ratio and fills the background with a specified color.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image): The image to be padded.\n",
    "        fill_color (tuple): The RGB color value used to fill the background. Default is black (0, 0, 0).\n",
    "\n",
    "    \"\"\"\n",
    "    max_dim = max(image.size)\n",
    "    new_img = Image.new(\"RGB\", (max_dim, max_dim), color=fill_color)\n",
    "    paste_position = ((max_dim - image.size[0]) // 2, (max_dim - image.size[1]) // 2)\n",
    "    new_img.paste(image, paste_position)\n",
    "    return new_img\n",
    "\n",
    "def process_images(source_folder, target_folder):\n",
    "    \"\"\"\n",
    "    Processes all images in the source folder by padding them to square, and saves the processed images to the target folder, maintaining the subfolder structure.\n",
    "\n",
    "    Args:\n",
    "        source_folder (str): The path to the source folder containing subfolders with images to be processed.\n",
    "        target_folder (str): The path to the target folder where processed images will be saved.\n",
    "    \"\"\"\n",
    "    for subdir in os.listdir(source_folder):\n",
    "        subdir_path = os.path.join(source_folder, subdir)\n",
    "        target_subdir = os.path.join(target_folder, subdir + '_padded')\n",
    "\n",
    "        if not os.path.exists(target_subdir):\n",
    "            os.makedirs(target_subdir)\n",
    "\n",
    "        for image_name in os.listdir(subdir_path):\n",
    "            source_path = os.path.join(subdir_path, image_name)\n",
    "            target_path = os.path.join(target_subdir, image_name)\n",
    "\n",
    "            if os.path.isfile(source_path): \n",
    "                try:\n",
    "                    with Image.open(source_path) as img:\n",
    "                        new_img = pad_image_to_square(img)\n",
    "                        new_img.save(target_path)\n",
    "                        print(f\"Processed and saved: {target_path}\")\n",
    "                except IOError:\n",
    "                    print(f\"Cannot open or process {image_name}\")\n",
    "\n",
    "# Paths to the source and target folders\n",
    "source_folder = 'Your Resized Image Path'\n",
    "target_folder = 'Your Target Image Path'\n",
    "\n",
    "# Process all images in the source folder and save them to the target folder\n",
    "process_images(source_folder, target_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the preprocessed CSV files into `train` and `val` datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pose_landmarks(csv_path):\n",
    "    \"\"\"\n",
    "    Loads a CSV created by MoveNetPreprocessor.\n",
    "\n",
    "    Returns:\n",
    "        X: Detected landmark coordinates and scores of shape (N, 17 * 3)\n",
    "        y: Ground truth labels of shape (N, label_count)\n",
    "        classes: The list of all class names found in the dataset\n",
    "        dataframe: The CSV loaded as a Pandas dataframe features (X) and ground\n",
    "        truth labels (y) to use later to train a pose classification model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV file\n",
    "    dataframe = pd.read_csv(csv_path)\n",
    "    df_to_process = dataframe.copy()\n",
    "\n",
    "    # Drop the file_name columns as you don't need it during training.\n",
    "    df_to_process.drop(columns=['file_name'], inplace=True)\n",
    "\n",
    "    # Extract the list of class names\n",
    "    classes = df_to_process.pop('class_name').unique()\n",
    "\n",
    "    # Extract the labels\n",
    "    y = df_to_process.pop('class_no')\n",
    "\n",
    "    # Convert the input features and labels into the correct format for training.\n",
    "    X = df_to_process.astype('float64')\n",
    "    y = keras.utils.to_categorical(y)\n",
    "\n",
    "    return X, y, classes, dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, class_names, _ = load_pose_landmarks('./Data/train.csv')\n",
    "X_val, y_val, class_names, _ = load_pose_landmarks('./Data/val.csv')\n",
    "X_test, y_test, _, df_test = load_pose_landmarks('./Data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_point(landmarks, left_bodypart, right_bodypart):\n",
    "  \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n",
    "\n",
    "  left = tf.gather(landmarks, left_bodypart.value, axis=1)\n",
    "  right = tf.gather(landmarks, right_bodypart.value, axis=1)\n",
    "  center = left * 0.5 + right * 0.5\n",
    "  return center\n",
    "\n",
    "\n",
    "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
    "  \"\"\"Calculates pose size.\n",
    "\n",
    "  It is the maximum of two values:\n",
    "    * Torso size multiplied by `torso_size_multiplier`\n",
    "    * Maximum distance from pose center to any pose landmark\n",
    "  \"\"\"\n",
    "  # Hips center\n",
    "  hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "\n",
    "  # Shoulders center\n",
    "  shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n",
    "                                      BodyPart.RIGHT_SHOULDER)\n",
    "\n",
    "  # Torso size as the minimum body size\n",
    "  torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
    "\n",
    "  # Pose center\n",
    "  pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                     BodyPart.RIGHT_HIP)\n",
    "  pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
    "  # Broadcast the pose center to the same size as the landmark vector to\n",
    "  # perform substraction\n",
    "  pose_center_new = tf.broadcast_to(pose_center_new,\n",
    "                                    [tf.size(landmarks) // (17*2), 17, 2])\n",
    "\n",
    "  # Dist to pose center\n",
    "  d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n",
    "                name=\"dist_to_pose_center\")\n",
    "  # Max dist to pose center\n",
    "  max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
    "\n",
    "  # Normalize scale\n",
    "  pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
    "\n",
    "  return pose_size\n",
    "\n",
    "\n",
    "def normalize_pose_landmarks(landmarks):\n",
    "  \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
    "  scaling it to a constant pose size.\n",
    "  \"\"\"\n",
    "  # Move landmarks so that the pose center becomes (0,0)\n",
    "  pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "  pose_center = tf.expand_dims(pose_center, axis=1)\n",
    "  # Broadcast the pose center to the same size as the landmark vector to perform\n",
    "  # substraction\n",
    "  pose_center = tf.broadcast_to(pose_center, \n",
    "                                [tf.size(landmarks) // (17*2), 17, 2])\n",
    "  landmarks = landmarks - pose_center\n",
    "\n",
    "  # Scale the landmarks to a constant pose size\n",
    "  pose_size = get_pose_size(landmarks)\n",
    "  landmarks /= pose_size\n",
    "\n",
    "  return landmarks\n",
    "\n",
    "\n",
    "def landmarks_to_embedding(landmarks_and_scores):\n",
    "  \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
    "  # Reshape the flat input into a matrix with shape=(17, 3)\n",
    "  reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
    "\n",
    "  # Normalize landmarks 2D\n",
    "  landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])\n",
    "\n",
    "  # Flatten the normalized landmark coordinates into a vector\n",
    "  embedding = keras.layers.Flatten()(landmarks)\n",
    "\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 51)]         0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 17, 3)        0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 17, 2)       0           ['reshape[0][0]']                \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather (TFOpLambd  (None, 2)           0           ['tf.__operators__.getitem[0][0]'\n",
      " a)                                                              ]                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_1 (TFOpLam  (None, 2)           0           ['tf.__operators__.getitem[0][0]'\n",
      " bda)                                                            ]                                \n",
      "                                                                                                  \n",
      " tf.math.multiply (TFOpLambda)  (None, 2)            0           ['tf.compat.v1.gather[0][0]']    \n",
      "                                                                                                  \n",
      " tf.math.multiply_1 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_1[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 2)           0           ['tf.math.multiply[0][0]',       \n",
      " da)                                                              'tf.math.multiply_1[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.size (TFOpLambda)  ()                  0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.expand_dims (TFOpLambda)    (None, 1, 2)         0           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div (TFOpLa  ()                  0           ['tf.compat.v1.size[0][0]']      \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " tf.broadcast_to (TFOpLambda)   (None, 17, 2)        0           ['tf.expand_dims[0][0]',         \n",
      "                                                                  'tf.compat.v1.floor_div[0][0]'] \n",
      "                                                                                                  \n",
      " tf.math.subtract (TFOpLambda)  (None, 17, 2)        0           ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 , 'tf.broadcast_to[0][0]']       \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_6 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_7 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.math.multiply_6 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_6[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_7 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_7[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 2)           0           ['tf.math.multiply_6[0][0]',     \n",
      " mbda)                                                            'tf.math.multiply_7[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.size_1 (TFOpLambd  ()                  0           ['tf.math.subtract[0][0]']       \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_4 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_5 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_2 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_3 (TFOpLam  (None, 2)           0           ['tf.math.subtract[0][0]']       \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.expand_dims_1 (TFOpLambda)  (None, 1, 2)         0           ['tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.floor_div_1 (TFOp  ()                  0           ['tf.compat.v1.size_1[0][0]']    \n",
      " Lambda)                                                                                          \n",
      "                                                                                                  \n",
      " tf.math.multiply_4 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_4[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_5 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_5[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_2 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_2[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.multiply_3 (TFOpLambda  (None, 2)           0           ['tf.compat.v1.gather_3[0][0]']  \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.broadcast_to_1 (TFOpLambda)  (None, 17, 2)       0           ['tf.expand_dims_1[0][0]',       \n",
      "                                                                  'tf.compat.v1.floor_div_1[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 2)           0           ['tf.math.multiply_4[0][0]',     \n",
      " mbda)                                                            'tf.math.multiply_5[0][0]']     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 2)           0           ['tf.math.multiply_2[0][0]',     \n",
      " mbda)                                                            'tf.math.multiply_3[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.subtract_2 (TFOpLambda  (None, 17, 2)       0           ['tf.math.subtract[0][0]',       \n",
      " )                                                                'tf.broadcast_to_1[0][0]']      \n",
      "                                                                                                  \n",
      " tf.math.subtract_1 (TFOpLambda  (None, 2)           0           ['tf.__operators__.add_2[0][0]', \n",
      " )                                                                'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.compat.v1.gather_8 (TFOpLam  (17, 2)             0           ['tf.math.subtract_2[0][0]']     \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm (TFOpLambda)  ()                  0           ['tf.math.subtract_1[0][0]']     \n",
      "                                                                                                  \n",
      " tf.compat.v1.norm_1 (TFOpLambd  (2,)                0           ['tf.compat.v1.gather_8[0][0]']  \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " tf.math.multiply_8 (TFOpLambda  ()                  0           ['tf.compat.v1.norm[0][0]']      \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.reduce_max (TFOpLambda  ()                  0           ['tf.compat.v1.norm_1[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tf.math.maximum (TFOpLambda)   ()                   0           ['tf.math.multiply_8[0][0]',     \n",
      "                                                                  'tf.math.reduce_max[0][0]']     \n",
      "                                                                                                  \n",
      " tf.math.truediv (TFOpLambda)   (None, 17, 2)        0           ['tf.math.subtract[0][0]',       \n",
      "                                                                  'tf.math.maximum[0][0]']        \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 34)           0           ['tf.math.truediv[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          4480        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2)            130         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 12,866\n",
      "Trainable params: 12,866\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "inputs = tf.keras.Input(shape=(51))\n",
    "embedding = landmarks_to_embedding(inputs)\n",
    "\n",
    "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(embedding)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "25/50 [==============>...............] - ETA: 0s - loss: 0.6883 - accuracy: 0.5475 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.73000, saving model to weights.best.hdf5\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.6755 - accuracy: 0.6087 - val_loss: 0.6347 - val_accuracy: 0.7300\n",
      "Epoch 2/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.6385 - accuracy: 0.7222\n",
      "Epoch 2: val_accuracy improved from 0.73000 to 0.90000, saving model to weights.best.hdf5\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6089 - accuracy: 0.7675 - val_loss: 0.5147 - val_accuracy: 0.9000\n",
      "Epoch 3/200\n",
      "26/50 [==============>...............] - ETA: 0s - loss: 0.5028 - accuracy: 0.8606\n",
      "Epoch 3: val_accuracy did not improve from 0.90000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4622 - accuracy: 0.8687 - val_loss: 0.3521 - val_accuracy: 0.9000\n",
      "Epoch 4/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.3664 - accuracy: 0.8906\n",
      "Epoch 4: val_accuracy did not improve from 0.90000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.9050 - val_loss: 0.2763 - val_accuracy: 0.9000\n",
      "Epoch 5/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.2652 - accuracy: 0.9259\n",
      "Epoch 5: val_accuracy improved from 0.90000 to 0.91000, saving model to weights.best.hdf5\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.9237 - val_loss: 0.2453 - val_accuracy: 0.9100\n",
      "Epoch 6/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.2297 - accuracy: 0.9306\n",
      "Epoch 6: val_accuracy did not improve from 0.91000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2201 - accuracy: 0.9300 - val_loss: 0.2369 - val_accuracy: 0.9100\n",
      "Epoch 7/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.1734 - accuracy: 0.9537\n",
      "Epoch 7: val_accuracy did not improve from 0.91000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1870 - accuracy: 0.9513 - val_loss: 0.2150 - val_accuracy: 0.9100\n",
      "Epoch 8/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.1785 - accuracy: 0.9537\n",
      "Epoch 8: val_accuracy did not improve from 0.91000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1667 - accuracy: 0.9575 - val_loss: 0.2213 - val_accuracy: 0.9100\n",
      "Epoch 9/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.1708 - accuracy: 0.9576\n",
      "Epoch 9: val_accuracy improved from 0.91000 to 0.95000, saving model to weights.best.hdf5\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.9625 - val_loss: 0.1798 - val_accuracy: 0.9500\n",
      "Epoch 10/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.1503 - accuracy: 0.9676\n",
      "Epoch 10: val_accuracy did not improve from 0.95000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1539 - accuracy: 0.9575 - val_loss: 0.1885 - val_accuracy: 0.9300\n",
      "Epoch 11/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.1568 - accuracy: 0.9576\n",
      "Epoch 11: val_accuracy did not improve from 0.95000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1320 - accuracy: 0.9663 - val_loss: 0.1747 - val_accuracy: 0.9500\n",
      "Epoch 12/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.1283 - accuracy: 0.9643\n",
      "Epoch 12: val_accuracy did not improve from 0.95000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1388 - accuracy: 0.9613 - val_loss: 0.1811 - val_accuracy: 0.9400\n",
      "Epoch 13/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.1386 - accuracy: 0.9665\n",
      "Epoch 13: val_accuracy improved from 0.95000 to 0.96000, saving model to weights.best.hdf5\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1309 - accuracy: 0.9675 - val_loss: 0.1363 - val_accuracy: 0.9600\n",
      "Epoch 14/200\n",
      "26/50 [==============>...............] - ETA: 0s - loss: 0.1277 - accuracy: 0.9639\n",
      "Epoch 14: val_accuracy did not improve from 0.96000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1159 - accuracy: 0.9700 - val_loss: 0.1485 - val_accuracy: 0.9600\n",
      "Epoch 15/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.1303 - accuracy: 0.9630\n",
      "Epoch 15: val_accuracy did not improve from 0.96000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1153 - accuracy: 0.9700 - val_loss: 0.1369 - val_accuracy: 0.9600\n",
      "Epoch 16/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0971 - accuracy: 0.9777\n",
      "Epoch 16: val_accuracy did not improve from 0.96000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1095 - accuracy: 0.9712 - val_loss: 0.1409 - val_accuracy: 0.9600\n",
      "Epoch 17/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.1209 - accuracy: 0.9630\n",
      "Epoch 17: val_accuracy did not improve from 0.96000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1109 - accuracy: 0.9700 - val_loss: 0.1376 - val_accuracy: 0.9600\n",
      "Epoch 18/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0853 - accuracy: 0.9754\n",
      "Epoch 18: val_accuracy did not improve from 0.96000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1006 - accuracy: 0.9688 - val_loss: 0.1197 - val_accuracy: 0.9600\n",
      "Epoch 19/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.1303 - accuracy: 0.9643\n",
      "Epoch 19: val_accuracy did not improve from 0.96000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1201 - accuracy: 0.9663 - val_loss: 0.1263 - val_accuracy: 0.9600\n",
      "Epoch 20/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.1104 - accuracy: 0.9621\n",
      "Epoch 20: val_accuracy did not improve from 0.96000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.1079 - accuracy: 0.9688 - val_loss: 0.1334 - val_accuracy: 0.9600\n",
      "Epoch 21/200\n",
      "26/50 [==============>...............] - ETA: 0s - loss: 0.1007 - accuracy: 0.9736\n",
      "Epoch 21: val_accuracy did not improve from 0.96000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0926 - accuracy: 0.9737 - val_loss: 0.1420 - val_accuracy: 0.9600\n",
      "Epoch 22/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.0944 - accuracy: 0.9769\n",
      "Epoch 22: val_accuracy improved from 0.96000 to 0.97000, saving model to weights.best.hdf5\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.1008 - accuracy: 0.9725 - val_loss: 0.1137 - val_accuracy: 0.9700\n",
      "Epoch 23/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.0701 - accuracy: 0.9861\n",
      "Epoch 23: val_accuracy did not improve from 0.97000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0915 - accuracy: 0.9812 - val_loss: 0.1272 - val_accuracy: 0.9700\n",
      "Epoch 24/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0422 - accuracy: 0.9866\n",
      "Epoch 24: val_accuracy improved from 0.97000 to 0.98000, saving model to weights.best.hdf5\n",
      "50/50 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9762 - val_loss: 0.1060 - val_accuracy: 0.9800\n",
      "Epoch 25/200\n",
      "26/50 [==============>...............] - ETA: 0s - loss: 0.0873 - accuracy: 0.9784\n",
      "Epoch 25: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0997 - accuracy: 0.9750 - val_loss: 0.1068 - val_accuracy: 0.9700\n",
      "Epoch 26/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.0721 - accuracy: 0.9792\n",
      "Epoch 26: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0781 - accuracy: 0.9787 - val_loss: 0.1048 - val_accuracy: 0.9700\n",
      "Epoch 27/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.0928 - accuracy: 0.9769\n",
      "Epoch 27: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.9775 - val_loss: 0.0983 - val_accuracy: 0.9700\n",
      "Epoch 28/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0856 - accuracy: 0.9754\n",
      "Epoch 28: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9775 - val_loss: 0.1269 - val_accuracy: 0.9600\n",
      "Epoch 29/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.0780 - accuracy: 0.9838\n",
      "Epoch 29: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9812 - val_loss: 0.0959 - val_accuracy: 0.9800\n",
      "Epoch 30/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.0905 - accuracy: 0.9745\n",
      "Epoch 30: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0894 - accuracy: 0.9787 - val_loss: 0.0971 - val_accuracy: 0.9700\n",
      "Epoch 31/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.0965 - accuracy: 0.9815\n",
      "Epoch 31: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0833 - accuracy: 0.9837 - val_loss: 0.1099 - val_accuracy: 0.9700\n",
      "Epoch 32/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0723 - accuracy: 0.9799\n",
      "Epoch 32: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0779 - accuracy: 0.9762 - val_loss: 0.0951 - val_accuracy: 0.9700\n",
      "Epoch 33/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0632 - accuracy: 0.9821\n",
      "Epoch 33: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0824 - accuracy: 0.9762 - val_loss: 0.0924 - val_accuracy: 0.9700\n",
      "Epoch 34/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0936 - accuracy: 0.9799\n",
      "Epoch 34: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0786 - accuracy: 0.9837 - val_loss: 0.1014 - val_accuracy: 0.9700\n",
      "Epoch 35/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0909 - accuracy: 0.9732\n",
      "Epoch 35: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0843 - accuracy: 0.9762 - val_loss: 0.1079 - val_accuracy: 0.9600\n",
      "Epoch 36/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0811 - accuracy: 0.9821\n",
      "Epoch 36: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0809 - accuracy: 0.9812 - val_loss: 0.0834 - val_accuracy: 0.9800\n",
      "Epoch 37/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.0544 - accuracy: 0.9884\n",
      "Epoch 37: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0737 - accuracy: 0.9787 - val_loss: 0.0863 - val_accuracy: 0.9800\n",
      "Epoch 38/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0815 - accuracy: 0.9799\n",
      "Epoch 38: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0791 - accuracy: 0.9800 - val_loss: 0.0858 - val_accuracy: 0.9700\n",
      "Epoch 39/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0481 - accuracy: 0.9888\n",
      "Epoch 39: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.9850 - val_loss: 0.1090 - val_accuracy: 0.9700\n",
      "Epoch 40/200\n",
      "27/50 [===============>..............] - ETA: 0s - loss: 0.0497 - accuracy: 0.9861\n",
      "Epoch 40: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0652 - accuracy: 0.9825 - val_loss: 0.0874 - val_accuracy: 0.9700\n",
      "Epoch 41/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0601 - accuracy: 0.9821\n",
      "Epoch 41: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.9775 - val_loss: 0.0839 - val_accuracy: 0.9800\n",
      "Epoch 42/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0725 - accuracy: 0.9754\n",
      "Epoch 42: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.9737 - val_loss: 0.0828 - val_accuracy: 0.9800\n",
      "Epoch 43/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0678 - accuracy: 0.9844\n",
      "Epoch 43: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0722 - accuracy: 0.9812 - val_loss: 0.0843 - val_accuracy: 0.9800\n",
      "Epoch 44/200\n",
      "28/50 [===============>..............] - ETA: 0s - loss: 0.0652 - accuracy: 0.9799\n",
      "Epoch 44: val_accuracy did not improve from 0.98000\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.0727 - accuracy: 0.9762 - val_loss: 0.0946 - val_accuracy: 0.9700\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Add a checkpoint callback to store the checkpoint that has the highest\n",
    "# validation accuracy.\n",
    "checkpoint_path = \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                              patience=20)\n",
    "\n",
    "# Start training\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the training history to see whether you're overfitting.\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['TRAIN', 'VAL'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4342 - accuracy: 0.7200\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\tmpnbbebtt5\\assets\n",
      "Model size: 26KB\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "print('Model size: %dKB' % (len(tflite_model) / 1024))\n",
    "\n",
    "with open('pose_classifier.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pose_labels.txt', 'w') as f:\n",
    "  f.write('\\n'.join(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of TFLite model: 0.86\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(interpreter, X, y_true):\n",
    "  \"\"\"Evaluates the given TFLite model and return its accuracy.\"\"\"\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on all given poses.\n",
    "  y_pred = []\n",
    "  for i in range(len(y_true)):\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = X[i: i + 1].astype('float32')\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the class with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    predicted_label = np.argmax(output()[0])\n",
    "    y_pred.append(predicted_label)\n",
    "\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  y_pred = keras.utils.to_categorical(y_pred)\n",
    "  return accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Evaluate the accuracy of the converted TFLite model\n",
    "classifier_interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "classifier_interpreter.allocate_tensors()\n",
    "print('Accuracy of TFLite model: %s' %\n",
    "      evaluate_model(classifier_interpreter, X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
